# -*- coding: utf-8 -*-
"""prefill & decode.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/132ylaY4RKxHUC7LKA8XfyRh6XwXw80YS
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

prompt = "Explain retrieval-augmented generation in one sentence.\n\n"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# Forward pass through the full prompt
# This builds the KV cache
with torch.no_grad():
    prefill_outputs = model(
        **inputs,
        use_cache=True
    )

kv_cache = prefill_outputs.past_key_values

# Start decoding with the last token of the prompt
last_token = inputs["input_ids"][:, -1:]

#decoding
generated_tokens = []
max_new_tokens = 30

with torch.no_grad():
    for _ in range(max_new_tokens):
        outputs = model(
            input_ids=last_token,
            past_key_values=kv_cache,
            use_cache=True
        )

        # Extract updated KV cache for next step
        kv_cache = outputs.past_key_values

        # Greedy decoding (choose highest prob token)
        next_token = torch.argmax(outputs.logits[:, -1, :], dim=-1, keepdim=True)

        generated_tokens.append(next_token.item())

        # Update last_token for next iteration
        last_token = next_token

        # Stop at EOS if needed
        if next_token.item() == tokenizer.eos_token_id:
            break

# Decode output
decoded_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)

print("Generated:", decoded_text)