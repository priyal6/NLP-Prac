# -*- coding: utf-8 -*-
"""tokenization and padding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17vIGblmmcTPEMtZm91tlXGzwYeuNxyaF
"""

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sentence = "The cat sat on the mat."
tokens = tokenizer.tokenize(sentence)
print(tokens)

token_ids = tokenizer.encode(sentence)
print(token_ids)

uknw = "unbelievable"
tokens = tokenizer.tokenize(uknw)
print(tokens)

decoded_sentence = tokenizer.decode(token_ids)
print(decoded_sentence)

sentences = [
"Pretraining improves performance.",
"Attention mechanisms are powerful.",
"Embeddings capture meaning.",
"Fine-tuning adapts models effectively.",
]

batch = tokenizer(sentences, padding=True)
print(batch)

for ids in batch["input_ids"]:
  print(tokenizer.decode(ids))